{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1de8cedf-f817-423d-b53c-1f4ec4d1454b",
      "metadata": {
        "id": "1de8cedf-f817-423d-b53c-1f4ec4d1454b"
      },
      "source": [
        "# Stochastic Optimization\n",
        "\n",
        "## Exercise on Control\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/stochastic/\">https://supaerodatascience.github.io/stochastic/</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92038fa",
      "metadata": {
        "id": "b92038fa"
      },
      "source": [
        "### **NAMES** : Paul Laisné, Niels Albrecht"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a218173a-8c0c-48f2-a36c-d13ad5de6461",
      "metadata": {
        "id": "a218173a-8c0c-48f2-a36c-d13ad5de6461"
      },
      "source": [
        "Today you'll explore advanced SGD methods (momentum, Adam) and compare them with gradient-free approaches on a robotic arm control problem. You'll also learn automatic differentiation with JAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760934ed-03db-41b4-b510-9241851ae253",
      "metadata": {
        "id": "760934ed-03db-41b4-b510-9241851ae253"
      },
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "from typing import Tuple, List, Callable\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Enable JAX to run on CPU (for consistent behavior across platforms)\n",
        "jax.config.update('jax_platform_name', 'cpu')\n",
        "\n",
        "print(\"Setup complete! JAX version:\", jax.__version__)\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e696a118-acec-4929-9eec-22187022c7f2",
      "metadata": {
        "id": "e696a118-acec-4929-9eec-22187022c7f2"
      },
      "source": [
        "# Exploring the Robot Arm\n",
        "Run the provided visualization to understand the 2-joint planar arm. Experiment with different joint angles. We will experiment with two different libraries for this robot arm: numpy and [jax](docs.jax.dev/en/latest/index.html). Jax will enable easier gradient calculations through [automatic differentiation](https://huggingface.co/blog/andmholm/what-is-automatic-differentiation). For now, understand and explore this 2-joint robot arm which uses two angles to calculate the end position of the arm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c89ae0-873d-4456-8e5c-9c2ad6d81363",
      "metadata": {
        "id": "91c89ae0-873d-4456-8e5c-9c2ad6d81363"
      },
      "outputs": [],
      "source": [
        "def forward_kinematics_numpy(theta: np.ndarray, lengths: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute end-effector position for a 2-joint planar arm.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta : np.ndarray, shape (2,)\n",
        "        Joint angles [theta1, theta2] in radians\n",
        "    lengths : np.ndarray, shape (2,)\n",
        "        Link lengths [L1, L2]\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    position : np.ndarray, shape (2,)\n",
        "        End-effector position [x, y]\n",
        "    \"\"\"\n",
        "    theta1, theta2 = theta\n",
        "    L1, L2 = lengths\n",
        "\n",
        "    # First joint position\n",
        "    x1 = L1 * np.cos(theta1)\n",
        "    y1 = L1 * np.sin(theta1)\n",
        "\n",
        "    # End-effector position\n",
        "    x2 = x1 + L2 * np.cos(theta1 + theta2)\n",
        "    y2 = y1 + L2 * np.sin(theta1 + theta2)\n",
        "\n",
        "    return np.array([x2, y2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_traj(k, target, trajs_out, etas, title=\"Trajectory\"):\n",
        "    traj = trajs_out[k]\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.plot(0, 0, linestyle=\"\", marker=\"+\", color=\"black\", markersize=16)\n",
        "\n",
        "    plt.plot(traj[:, 0], traj[:, 1], label=f\"Traj for η={etas[k]}\", linewidth=2, color=\"dodgerblue\")\n",
        "    plt.plot(target[0], target[1], linestyle=\"\", marker=\"*\", label=\"Target\", color=\"crimson\", markersize=16)\n",
        "    plt.plot(traj[0, 0], traj[0, 1], linestyle=\"\", marker=\"o\", label=f\"Starting point\", linewidth=2, color=\"grey\", markersize=6)\n",
        "    plt.plot(traj[-1, 0], traj[-1, 1], linestyle=\"\", marker=\"o\", label=f\"Ending point (η={etas[k]})\", linewidth=2, color=\"orange\", markersize=6)\n",
        "    plt.grid()\n",
        "    plt.xlim((-2, 2))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_multitraj(target, trajs, labels, title=\"Trajectory\"):\n",
        "    plt.figure(figsize=(12, 9))\n",
        "    plt.title(title)\n",
        "    plt.plot(0, 0, linestyle=\"\", marker=\"+\", color=\"black\", markersize=16)\n",
        "    colors=[\"dodgerblue\", \"darkblue\", \"royalblue\", \"orange\"]\n",
        "\n",
        "    plt.plot(target[0], target[1], linestyle=\"\", marker=\"*\", label=\"Target\", color=\"crimson\", markersize=16)\n",
        "    for i, traj in enumerate(trajs):\n",
        "        plt.plot(traj[:, 0], traj[:, 1], label=f\"Traj of {labels[i]}\", linewidth=2, color=colors[i], alpha=0.7)\n",
        "        plt.plot(traj[-1, 0], traj[-1, 1], linestyle=\"\", marker=\"o\", label=f\"Ending point ({labels[i]})\", linewidth=2, color=colors[i], markersize=6, alpha=0.7)\n",
        "    plt.plot(trajs[0][0, 0], trajs[0][0, 1], linestyle=\"\", marker=\"o\", label=f\"Starting point\", linewidth=2, color=\"grey\", markersize=6)\n",
        "    plt.grid()\n",
        "    plt.xlim((-2, 2))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_theta(k, thetas, etas):\n",
        "    theta = thetas[k]\n",
        "    plt.figure()\n",
        "    plt.title(\"Evolution of thetas\")\n",
        "    plt.plot(0, 0, linestyle=\"\", marker=\"+\", color=\"black\", markersize=16)\n",
        "    plt.plot(theta[:, 0], label=f\"Theta Shoulder (η={etas[k]})\", linewidth=2, color=\"dodgerblue\")\n",
        "    plt.plot(theta[:, 1], label=f\"Theta Elbow (η={etas[k]})\", linewidth=2, color=\"magenta\")\n",
        "    plt.grid()\n",
        "    plt.ylim((-2*np.pi/3, 2*np.pi/3))\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"θ\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zjGnKN-WRSJz"
      },
      "id": "zjGnKN-WRSJz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12fefab2-358c-484c-a099-9539a5007e56",
      "metadata": {
        "id": "12fefab2-358c-484c-a099-9539a5007e56"
      },
      "outputs": [],
      "source": [
        "# JAX version (for automatic differentiation)\n",
        "def forward_kinematics_jax(theta: jnp.ndarray, lengths: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"\n",
        "    Compute end-effector position for a 2-joint planar arm (JAX version).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta : jnp.ndarray, shape (2,)\n",
        "        Joint angles [theta1, theta2] in radians\n",
        "    lengths : jnp.ndarray, shape (2,)\n",
        "        Link lengths [L1, L2]\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    position : jnp.ndarray, shape (2,)\n",
        "        End-effector position [x, y]\n",
        "    \"\"\"\n",
        "    theta1, theta2 = theta\n",
        "    L1, L2 = lengths\n",
        "\n",
        "    # First joint position\n",
        "    x1 = L1 * jnp.cos(theta1)\n",
        "    y1 = L1 * jnp.sin(theta1)\n",
        "\n",
        "    # End-effector position\n",
        "    x2 = x1 + L2 * jnp.cos(theta1 + theta2)\n",
        "    y2 = y1 + L2 * jnp.sin(theta1 + theta2)\n",
        "\n",
        "    return jnp.array([x2, y2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c430bcb2-173a-4c4d-9a8e-faa9070f92f4",
      "metadata": {
        "id": "c430bcb2-173a-4c4d-9a8e-faa9070f92f4"
      },
      "outputs": [],
      "source": [
        "# Get all joint positions for visualization\n",
        "def get_arm_points(theta: np.ndarray, lengths: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Get positions of all joints for visualization.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    points : np.ndarray, shape (3, 2)\n",
        "        Positions of [base, joint1, end-effector]\n",
        "    \"\"\"\n",
        "    theta1, theta2 = theta\n",
        "    L1, L2 = lengths\n",
        "\n",
        "    # Base at origin\n",
        "    p0 = np.array([0.0, 0.0])\n",
        "\n",
        "    # First joint\n",
        "    p1 = np.array([L1 * np.cos(theta1), L1 * np.sin(theta1)])\n",
        "\n",
        "    # End-effector\n",
        "    p2 = p1 + np.array([L2 * np.cos(theta1 + theta2),\n",
        "                        L2 * np.sin(theta1 + theta2)])\n",
        "\n",
        "    return np.array([p0, p1, p2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a219080b-95e7-4584-888e-b5b202b83a06",
      "metadata": {
        "id": "a219080b-95e7-4584-888e-b5b202b83a06"
      },
      "outputs": [],
      "source": [
        "# Test the implementations\n",
        "print(\"Testing forward kinematics...\")\n",
        "test_theta = np.array([np.pi/4, np.pi/4])\n",
        "test_lengths = np.array([1.0, 1.0])\n",
        "\n",
        "pos_numpy = forward_kinematics_numpy(test_theta, test_lengths)\n",
        "pos_jax = forward_kinematics_jax(test_theta, test_lengths)\n",
        "\n",
        "print(f\"NumPy result: {pos_numpy}\")\n",
        "print(f\"JAX result: {pos_jax}\")\n",
        "print(f\"Match: {np.allclose(pos_numpy, np.array(pos_jax))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca3c8607-9dfa-47af-b464-1acd5982acf9",
      "metadata": {
        "id": "ca3c8607-9dfa-47af-b464-1acd5982acf9"
      },
      "outputs": [],
      "source": [
        "# Visualization Utilities\n",
        "def plot_arm(theta: np.ndarray, lengths: np.ndarray,\n",
        "             target: np.ndarray = None, obstacles: List = None,\n",
        "             ax=None, title: str = \"Robot Arm\"):\n",
        "    \"\"\"\n",
        "    Plot the robot arm configuration.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta : np.ndarray\n",
        "        Joint angles\n",
        "    lengths : np.ndarray\n",
        "        Link lengths\n",
        "    target : np.ndarray, optional\n",
        "        Target position to visualize\n",
        "    obstacles : List of tuples, optional\n",
        "        List of (x, y, radius) for circular obstacles\n",
        "    ax : matplotlib axis, optional\n",
        "        Axis to plot on\n",
        "    title : str\n",
        "        Plot title\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "\n",
        "    # Get arm points\n",
        "    points = get_arm_points(theta, lengths)\n",
        "\n",
        "    # Plot arm links\n",
        "    ax.plot(points[:, 0], points[:, 1], 'o-', linewidth=3,\n",
        "            markersize=8, label='Arm', color='blue')\n",
        "\n",
        "    # Plot base\n",
        "    ax.plot(0, 0, 'ks', markersize=12, label='Base')\n",
        "\n",
        "    # Plot end-effector\n",
        "    ax.plot(points[-1, 0], points[-1, 1], 'ro',\n",
        "            markersize=10, label='End-effector')\n",
        "\n",
        "    # Plot target if provided\n",
        "    if target is not None:\n",
        "        ax.plot(target[0], target[1], 'g*',\n",
        "                markersize=15, label='Target')\n",
        "\n",
        "    # Plot obstacles if provided\n",
        "    if obstacles is not None:\n",
        "        for obs in obstacles:\n",
        "            circle = Circle((obs[0], obs[1]), obs[2],\n",
        "                          color='red', alpha=0.3, label='Obstacle')\n",
        "            ax.add_patch(circle)\n",
        "\n",
        "    # Set axis properties\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(-2.5, 2.5)\n",
        "    ax.set_ylim(-2.5, 2.5)\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729e56ee-db07-41ad-a568-6dcdffb0d959",
      "metadata": {
        "id": "729e56ee-db07-41ad-a568-6dcdffb0d959"
      },
      "outputs": [],
      "source": [
        "# Standard link lengths we'll use throughout\n",
        "LENGTHS = np.array([1.0, 1.0])\n",
        "\n",
        "# Example: Visualize different arm configurations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(10, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Some configurations\n",
        "example_configs = [\n",
        "    (np.array([0.0, 0.0]), \"Fully Extended\"),\n",
        "    (np.array([np.pi/2, 0.0]), \"Elbow Straight Up\"),\n",
        "    (np.array([np.pi/4, np.pi/4]), \"45° - 45°\"),\n",
        "    (np.array([0.0, np.pi/2]), \"Elbow Down\"),\n",
        "    (np.array([-np.pi/4, np.pi/2]), \"-45° - 90°\"),\n",
        "    (np.array([-np.pi/4, -np.pi/2]), \"-45° - 90° (Down)\")\n",
        "]\n",
        "\n",
        "target = np.array([1.1, 1.1])\n",
        "for i, (theta, title) in enumerate(example_configs):\n",
        "    plot_arm(theta, LENGTHS, target=target, ax=axes[i], title=title)\n",
        "    end_eff = forward_kinematics_numpy(theta, LENGTHS)\n",
        "    axes[i].text(0.05, 0.95, f'End-eff: ({end_eff[0]:.2f}, {end_eff[1]:.2f})',\n",
        "                transform=axes[i].transAxes, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22431f4b-a364-4075-971b-a8876ca70030",
      "metadata": {
        "id": "22431f4b-a364-4075-971b-a8876ca70030"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q1: The robot arm has multiple solutions to reach the same point (elbow-up vs elbow-down configurations). Using the provided forward_kinematics function, find two different joint angle configurations θ = [θ₁, θ₂] that place the end-effector at approximately (1.1, 1.1). What does this tell you about the optimization landscape?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38269bdc",
      "metadata": {
        "id": "38269bdc"
      },
      "source": [
        "**A1** : To find two different joint angle configurations that place the end-effector at approximately (1.1, 1.1), we can use the forward_kinematics function to calculate the end position of the arm for various joint angles.\n",
        "To implement this, we can iterate through a range of joint angles and check if the resulting end position is close to (1.1, 1.1).\n",
        "\n",
        "We will iterate through the following grid configurations:\n",
        "- $\\theta_1 \\in [-\\pi, \\pi]$ with 1000 samples\n",
        "- $\\theta_2 \\in [-\\pi, \\pi]$ with 1000 samples\n",
        "\n",
        "To check if the resulting end position is close to (1.1, 1.1), we can use a small tolerance value (e.g., $1e-5$) to determine if the distance between the calculated end position and the target position is within this tolerance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e63d3e-d44c-4ff7-be5f-65ad76a7c4e8",
      "metadata": {
        "id": "36e63d3e-d44c-4ff7-be5f-65ad76a7c4e8"
      },
      "outputs": [],
      "source": [
        "theta1_list = np.linspace(-np.pi, np.pi, 1000)\n",
        "theta2_list = np.linspace(-np.pi, np.pi, 1000)\n",
        "target = np.array([1.1, 1.1])\n",
        "valid_pos = []\n",
        "end_eff_pos = []\n",
        "tol = 1e-5\n",
        "best_pos = np.array([0.0, 0.0])\n",
        "best_distance = np.sum((target - best_pos) ** 2)\n",
        "\n",
        "for theta1 in theta1_list:\n",
        "    for theta2 in theta2_list:\n",
        "        theta = np.array([theta1, theta2])\n",
        "        end_pos = forward_kinematics_numpy(theta, LENGTHS)\n",
        "        # use elementwise power and sum to get squared distance\n",
        "        if np.sum((end_pos - target) ** 2) < tol:\n",
        "            valid_pos.append(theta)\n",
        "            end_eff_pos.append(end_pos)\n",
        "        if np.sum((end_pos - target) ** 2) < best_distance:\n",
        "            best_pos = theta\n",
        "            best_distance = np.sum((end_pos - target) ** 2)\n",
        "\n",
        "valid_pos = np.array(valid_pos)\n",
        "end_eff_pos = np.array(end_eff_pos)\n",
        "\n",
        "print(\"Valid positions:\")\n",
        "for pos in valid_pos:\n",
        "    print(\"Theta1: {:.4f}, Theta2: {:.4f}\".format(pos[0], pos[1]))\n",
        "print(\"End_effector corresponding positions:\")\n",
        "for pos in end_eff_pos:\n",
        "    print(\"X: {:.4f}, Y: {:.4f}\".format(pos[0], pos[1]))\n",
        "\n",
        "print(f\"Best position found: Theta1: {best_pos[0]:.4f}, Theta2: {best_pos[1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc210a5",
      "metadata": {
        "id": "3fc210a5"
      },
      "source": [
        "By chance, we found exactly with this method 2 configurations that satisfy this condition ! We will now visualize them below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3efe71",
      "metadata": {
        "id": "ee3efe71"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(len(valid_pos)//2 if len(valid_pos) % 2 == 0 else len(valid_pos)//2 + 1, 2)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, theta in enumerate(valid_pos):\n",
        "    print(i)\n",
        "    plot_arm(theta, LENGTHS, target=target, ax=axes[i], title=f\"pos{i}\")\n",
        "    end_eff = forward_kinematics_numpy(theta, LENGTHS)\n",
        "    axes[i].text(0.05, 0.95, f'End-eff: ({end_eff[0]:.2f}, {end_eff[1]:.2f})',\n",
        "                transform=axes[i].transAxes, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6137e6c7",
      "metadata": {
        "id": "6137e6c7"
      },
      "source": [
        "We can see that for any given target position inside the reachable workspace of the robot arm, there are typically two distinct joint angle configurations (elbow-up and elbow-down) that can achieve the same end-effector position. This indicates that the optimization landscape has multiple local minima corresponding to these different configurations.\n",
        "\n",
        "For the particular positions where $\\theta_2 = 0$, there is only one configuration (fully extended arm), which represents a unique solution in the optimization landscape."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9eb8550-af8a-44a9-b59f-eec5c21316d1",
      "metadata": {
        "id": "f9eb8550-af8a-44a9-b59f-eec5c21316d1"
      },
      "source": [
        "## Part 1: Automatic Differentiation\n",
        "\n",
        "Instead of guessing at the angles for a target, we will calculate them exactly. Use the provided JAX implementation to compute gradients of the loss function with respect to joint angles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f098564-027a-42ff-881f-fbe40782889a",
      "metadata": {
        "id": "9f098564-027a-42ff-881f-fbe40782889a"
      },
      "outputs": [],
      "source": [
        "# Loss Functions\n",
        "def loss_target_reaching(theta: jnp.ndarray, lengths: jnp.ndarray,\n",
        "                        target: jnp.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Loss for reaching a target position.\n",
        "\n",
        "    L = ||end_effector(theta) - target||^2\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta : jnp.ndarray, shape (2,)\n",
        "        Joint angles\n",
        "    lengths : jnp.ndarray, shape (2,)\n",
        "        Link lengths\n",
        "    target : jnp.ndarray, shape (2,)\n",
        "        Target position\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    loss : float\n",
        "        Squared distance to target\n",
        "    \"\"\"\n",
        "    end_effector = forward_kinematics_jax(theta, lengths)\n",
        "    return jnp.sum((end_effector - target) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92a99f8-f645-4a3b-8176-1459f81dfa88",
      "metadata": {
        "id": "b92a99f8-f645-4a3b-8176-1459f81dfa88"
      },
      "outputs": [],
      "source": [
        "# Create JIT-compiled versions for speed\n",
        "loss_target_reaching_jit = jit(loss_target_reaching)\n",
        "print(loss_target_reaching)\n",
        "print(loss_target_reaching_jit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58862ffa-2bdb-4158-aff9-17dd40c660ad",
      "metadata": {
        "id": "58862ffa-2bdb-4158-aff9-17dd40c660ad"
      },
      "outputs": [],
      "source": [
        "# Create gradient functions\n",
        "grad_loss_target = jit(grad(loss_target_reaching, argnums=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f9a3ed",
      "metadata": {
        "id": "37f9a3ed"
      },
      "outputs": [],
      "source": [
        "# Plot some loss landscapes\n",
        "theta1_vals = jnp.linspace(-jnp.pi, jnp.pi, 300)\n",
        "theta2_vals = jnp.linspace(-jnp.pi, jnp.pi, 300)\n",
        "Theta1, Theta2 = jnp.meshgrid(theta1_vals, theta2_vals)\n",
        "target = jnp.array([1.1, 1.1])\n",
        "Loss_vals = jnp.zeros(Theta1.shape)\n",
        "for i in range(Theta1.shape[0]):\n",
        "    for j in range(Theta1.shape[1]):\n",
        "        theta = jnp.array([Theta1[i, j], Theta2[i, j]])\n",
        "        Loss_vals = Loss_vals.at[i, j].set(loss_target_reaching_jit(theta, LENGTHS, target))\n",
        "\n",
        "# Plot loss landscape\n",
        "plt.figure()\n",
        "cp = plt.contourf(Theta1, Theta2, Loss_vals, levels=50, cmap='viridis')\n",
        "plt.colorbar(cp)\n",
        "plt.xlabel('Theta 1 (radians)')\n",
        "plt.ylabel('Theta 2 (radians)')\n",
        "plt.title('Loss Landscape for Target Reaching')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f015636-40f8-430c-bc64-8abba3b0f158",
      "metadata": {
        "id": "0f015636-40f8-430c-bc64-8abba3b0f158"
      },
      "outputs": [],
      "source": [
        "# Test the loss functions\n",
        "print(\"Testing loss functions...\")\n",
        "test_theta = jnp.array([0.5, 0.5])\n",
        "test_lengths = jnp.array([1.0, 1.0])\n",
        "test_target = jnp.array([1.0, 1.0])\n",
        "\n",
        "loss_val = loss_target_reaching(test_theta, test_lengths, test_target)\n",
        "print(f\"Loss value: {loss_val:.4f}\")\n",
        "\n",
        "grad_val = grad_loss_target(test_theta, test_lengths, test_target)\n",
        "print(f\"Gradient: {grad_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a2a0ec-7f97-43d5-95f8-b76f834fc13d",
      "metadata": {
        "id": "58a2a0ec-7f97-43d5-95f8-b76f834fc13d"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q2: Compute the gradient using JAX autodiff at $θ=[π/6, π/4]$\n",
        "Then verify it matches a manual finite-difference approximation:\n",
        "\n",
        "For finite differences, use:\n",
        "  $∂L/∂θᵢ ≈ (L(θ + h*eᵢ) - L(θ - h*eᵢ)) / (2h)$\n",
        "  where $eᵢ$ is the i-th unit vector and $h = 1e-5$\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c7c5d5",
      "metadata": {
        "id": "04c7c5d5"
      },
      "outputs": [],
      "source": [
        "test_theta = jnp.array([np.pi/6, np.pi/4])\n",
        "grad_val_jax =grad_loss_target(test_theta, test_lengths, test_target)\n",
        "h = 1e-5\n",
        "finit_diff_approx_1 = (loss_target_reaching(test_theta + np.array([h, 0]), test_lengths, test_target) - loss_target_reaching(test_theta - np.array([h, 0]), test_lengths, test_target))/(2*h)\n",
        "finit_diff_approx_2 = (loss_target_reaching(test_theta + np.array([0, h]), test_lengths, test_target) - loss_target_reaching(test_theta - np.array([0, h]), test_lengths, test_target))/(2*h)\n",
        "finit_diff_approx = np.array([finit_diff_approx_1, finit_diff_approx_2])\n",
        "print(grad_val_jax)\n",
        "print(finit_diff_approx)\n",
        "print(f\"Match: {np.allclose(finit_diff_approx, np.array(grad_val_jax))} \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2: The gradient computed by JAX matches the finite difference gradient."
      ],
      "metadata": {
        "id": "hf_JSpWzaHV4"
      },
      "id": "hf_JSpWzaHV4"
    },
    {
      "cell_type": "markdown",
      "id": "e88da9fc-f166-4eed-bd60-0ec66ad52e34",
      "metadata": {
        "id": "e88da9fc-f166-4eed-bd60-0ec66ad52e34"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q3: Compare the computational time of evaluating the gradient using JAX autodiff versus evaluating just the forward pass (loss calculation only). Run each 1000 times and report the ratio. What does this tell you about the computational overhead of automatic differentiation?</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f594d7d2",
      "metadata": {
        "id": "f594d7d2"
      },
      "outputs": [],
      "source": [
        "start = time.perf_counter()\n",
        "for _ in range(1000):\n",
        "    # jit\n",
        "    grad_loss_target(test_theta, test_lengths, test_target)\n",
        "duration_autodiff = (time.perf_counter() - start)\n",
        "\n",
        "start = time.perf_counter()\n",
        "for _ in range(1000):\n",
        "    loss_target_reaching(test_theta, test_lengths, test_target)\n",
        "duration_loss = (time.perf_counter() - start)\n",
        "\n",
        "start = time.perf_counter()\n",
        "for _ in range(1000):\n",
        "    loss_target_reaching_jit(test_theta, test_lengths, test_target)\n",
        "duration_loss_jit = (time.perf_counter() - start)\n",
        "\n",
        "print(\"Autodiff :\", duration_autodiff)\n",
        "print(\"Loss target without jit\", duration_loss)\n",
        "print(\"Loss target with jit \", duration_loss_jit)\n",
        "print(\"Ratio of Loss Target : \", duration_loss_jit/duration_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3: JIT loss target is roughly 600 times faster than the standard loss target function. It seems to depend a lot on the computer use, tho."
      ],
      "metadata": {
        "id": "suY264EOaViQ"
      },
      "id": "suY264EOaViQ"
    },
    {
      "cell_type": "markdown",
      "id": "1b99be32-1927-4e98-a61b-2af42210011f",
      "metadata": {
        "id": "1b99be32-1927-4e98-a61b-2af42210011f"
      },
      "source": [
        "## Part 2: Gradient Descent\n",
        "\n",
        "Gradient Descent (GD) is the foundational optimization algorithm that iteratively moves parameters in the direction of steepest descent:\n",
        "$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$\n",
        "\n",
        "While conceptually simple, GD can be sensitive to the learning rate $η$ and may struggle with saddle points or narrow valleys in the loss landscape where the gradient direction changes rapidly.\n",
        "\n",
        "For this exercise, implement basic gradient descent to move the robot arm to a target position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be08b645-483c-4492-9cc4-4159d053d6c4",
      "metadata": {
        "id": "be08b645-483c-4492-9cc4-4159d053d6c4"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(theta_init: np.ndarray,\n",
        "                     target: np.ndarray,\n",
        "                     lengths: np.ndarray,\n",
        "                     learning_rate: float = 0.01,\n",
        "                     n_iterations: int = 500,\n",
        "                     lambda_obs: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Convert to JAX arrays\n",
        "    theta = jnp.array(theta_init)\n",
        "    lengths_jax = jnp.array(lengths)\n",
        "    target_jax = jnp.array(target)\n",
        "    reach_iter = np.inf\n",
        "\n",
        "\n",
        "    loss_fn = lambda th: loss_target_reaching(th, lengths_jax, target_jax)\n",
        "    grad_fn = grad_loss_target\n",
        "\n",
        "    # Storage for history\n",
        "    theta_history = np.zeros((n_iterations + 1, 2))\n",
        "    loss_history = np.zeros(n_iterations + 1)\n",
        "    ee_trajectory = np.zeros((n_iterations + 1, 2))\n",
        "\n",
        "    # Initial values\n",
        "    theta_history[0] = np.array(theta)\n",
        "    loss_history[0] = float(loss_fn(theta))\n",
        "    ee_trajectory[0] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in range(n_iterations):\n",
        "        # TODO: Compute gradient\n",
        "        # gradient = ?\n",
        "        grad = grad_fn(theta, lengths_jax, target_jax)\n",
        "        # TODO: Update\n",
        "        # theta using gradient descent rule\n",
        "        # theta = ?\n",
        "        theta = theta - learning_rate * grad\n",
        "        # Store history\n",
        "        theta_history[i + 1] = np.array(theta)\n",
        "        loss_history[i + 1] = float(loss_fn(theta))\n",
        "        ee_trajectory[i + 1] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "        if reach_iter > 1000:\n",
        "            if np.sqrt(np.sum((ee_trajectory[i + 1] - target_jax)**2)) < 1e-2:\n",
        "                reach_iter = i + 1\n",
        "\n",
        "    return theta_history, loss_history, ee_trajectory, reach_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446e55d0-6b19-4547-8c8a-d84c5d2e1181",
      "metadata": {
        "id": "446e55d0-6b19-4547-8c8a-d84c5d2e1181"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q4: Run GD with learning rates $η = [0.001, 0.01, 0.1, 0.5, 1.0]$ for 500 iterations each, starting from $θ₀ = [0.1, 0.1]$ targeting (0.5, 1.5). Plot the loss curves. At what learning rate do you observe divergence? Explain what's happening in terms of the gradient descent update rule.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3fde23",
      "metadata": {
        "id": "ff3fde23"
      },
      "outputs": [],
      "source": [
        "learning_rate = [1e-3, 1e-2, 0.1, 0.5, 1.0]\n",
        "n_iter = 500\n",
        "theta0 = np.array([0.1, 0.1])\n",
        "target = np.array([0.5, 1.5])\n",
        "histories = {}\n",
        "for lr in learning_rate:\n",
        "    histories[lr] = gradient_descent(theta0, target, LENGTHS, lr, n_iter)\n",
        "\n",
        "plt.figure()\n",
        "for lr in learning_rate:\n",
        "    plt.plot(histories[lr][1], label=f\"{lr}\")\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A4: Divergence is observed for $\\eta > 0.5$. When the learning rate is too high, the arm never quite reach the target and instead oscillate around it. At every update step, given the high learning rate, it performs a important movement in the right direction, but overshoot its target. Therefore, it never gets close enough from its target."
      ],
      "metadata": {
        "id": "di4NBZj7cCbK"
      },
      "id": "di4NBZj7cCbK"
    },
    {
      "cell_type": "markdown",
      "id": "b047ff70-465b-44c1-bc55-ca15c349190f",
      "metadata": {
        "id": "b047ff70-465b-44c1-bc55-ca15c349190f"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q5: For $η = 0.01$, plot the end-effector trajectory in 2D space (not the loss, but the actual path the end-effector takes). Does it take the most direct path to the target? Why or why not? (Hint: think about parameter space vs task space)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48a70d6",
      "metadata": {
        "id": "b48a70d6"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "# print(histories[lr][2])\n",
        "# plt.figure(figsize=(10,10))\n",
        "# plt.plot(histories[lr][2][:, 0], histories[lr][2][:, 1], label=f\"{lr}\")\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "plot_multitraj(target=target, trajs=[histories[lr][2]], labels=[f\"Gradient descent, lr={lr}\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5: The arm does not take the most effective path to the target. By moving its shoulder, it reduces Loss a lot more than by moving its elbow. So, from a Loss perspective, getting quite close using mostly the shoulder is very efficient. However, in terms of trajectory, it is not the most direct path.\n",
        "\n",
        "From a Loss space perspective, there must be a very steep descent in the direction of $\\theta_1$, and a shallower but more direct one corresponding to a movement of both joints."
      ],
      "metadata": {
        "id": "nvakw41sdsvI"
      },
      "id": "nvakw41sdsvI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf700009",
      "metadata": {
        "id": "cf700009"
      },
      "outputs": [],
      "source": [
        "def animate_robot(history):\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "    # Normalize input: accept dict (histories), tuple/list (history tuple), or raw theta_history array\n",
        "    if isinstance(history, dict):\n",
        "        first = next(iter(history.values()))\n",
        "        theta_hist = first[0] if isinstance(first, (list, tuple)) else first\n",
        "    elif isinstance(history, (list, tuple)):\n",
        "        theta_hist = history[0]\n",
        "    else:\n",
        "        theta_hist = history\n",
        "\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        theta = theta_hist[frame]\n",
        "        plot_arm(theta, LENGTHS, target=target, ax=ax, title=f\"Iteration {frame}\")\n",
        "        print(f\"Frame {frame}: Theta = {theta}, End-effector = {forward_kinematics_numpy(theta, LENGTHS)}\")\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(theta_hist), interval=100)\n",
        "    plt.close(fig)\n",
        "    return ani\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8912cba",
      "metadata": {
        "id": "c8912cba"
      },
      "outputs": [],
      "source": [
        "# Run animation\n",
        "ani = animate_robot(histories[lr])\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fdcd034-bd9f-4ae4-a8db-a6eb9f1951d6",
      "metadata": {
        "id": "7fdcd034-bd9f-4ae4-a8db-a6eb9f1951d6"
      },
      "source": [
        "## 2.2: SGD with Momentum\n",
        "\n",
        "Momentum adds \"inertia\" to gradient descent by accumulating a velocity vector that combines the current gradient with previous gradients: $_t = \\beta v_{t-1} + \\nabla L(\\theta_t)$\n",
        ", then $\\theta_{t+1} = \\theta_t - \\eta v_t$. This helps the optimizer build up speed in consistent directions and dampens oscillations in directions where gradients frequently change sign, allowing it to better navigate ravines and escape shallow local minima.\n",
        "\n",
        "\n",
        "For this exercise, implement momentum-based gradient descent using the update rules:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c380f1-40f1-4322-821d-3c87dfc169ab",
      "metadata": {
        "id": "59c380f1-40f1-4322-821d-3c87dfc169ab"
      },
      "source": [
        "$v_t = βv_{t-1} + ∇L(θ_t)$\n",
        "\n",
        "$θ_{t+1} = θ_t - ηv_t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd54d6a",
      "metadata": {
        "id": "3fd54d6a"
      },
      "outputs": [],
      "source": [
        "def Momentum(theta_init: np.ndarray,\n",
        "                     target: np.ndarray,\n",
        "                     lengths: np.ndarray,\n",
        "                     learning_rate: float = 0.01,\n",
        "                     n_iterations: int = 500,\n",
        "                     beta: float = 0.9,\n",
        "                     lambda_obs: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Convert to JAX arrays\n",
        "    theta = jnp.array(theta_init)\n",
        "    lengths_jax = jnp.array(lengths)\n",
        "    target_jax = jnp.array(target)\n",
        "    v = jnp.array([0, 0])\n",
        "    reach_iter = np.inf\n",
        "\n",
        "    loss_fn = lambda th: loss_target_reaching(th, lengths_jax, target_jax)\n",
        "    grad_fn = grad_loss_target\n",
        "\n",
        "    # Storage for history\n",
        "    theta_history = np.zeros((n_iterations + 1, 2))\n",
        "    loss_history = np.zeros(n_iterations + 1)\n",
        "    ee_trajectory = np.zeros((n_iterations + 1, 2))\n",
        "    v_history = np.zeros((n_iterations + 1, 2))\n",
        "\n",
        "    # Initial values\n",
        "    theta_history[0] = np.array(theta)\n",
        "    loss_history[0] = float(loss_fn(theta))\n",
        "    ee_trajectory[0] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "    v_history[0] = np.array(v)\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in range(n_iterations):\n",
        "        # TODO: Compute gradient\n",
        "        # gradient = ?\n",
        "        grad = grad_fn(theta, lengths_jax, target_jax)\n",
        "        v = beta * v + grad\n",
        "        # TODO: Update\n",
        "        # theta using gradient descent rule\n",
        "        # theta = ?\n",
        "        theta = theta - learning_rate * v\n",
        "        # Store history\n",
        "        theta_history[i + 1] = np.array(theta)\n",
        "        loss_history[i + 1] = float(loss_fn(theta))\n",
        "        ee_trajectory[i + 1] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "        v_history[i+1] = np.array(v)\n",
        "        if reach_iter > 1000:\n",
        "            if np.sqrt(np.sum((ee_trajectory[i + 1] - target_jax)**2)) < 1e-2:\n",
        "                reach_iter = i + 1\n",
        "\n",
        "    return theta_history, loss_history, ee_trajectory, v_history, reach_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6d51d0",
      "metadata": {
        "id": "bf6d51d0"
      },
      "outputs": [],
      "source": [
        "learning_rate = [1e-3, 1e-2, 0.1]\n",
        "n_iter = 500\n",
        "theta0 = np.array([0.1, 0.1])\n",
        "target = np.array([0.5, 1.5])\n",
        "histories_momentum = {}\n",
        "for lr in learning_rate:\n",
        "    histories_momentum[lr] = Momentum(theta0, target, LENGTHS, lr, n_iter)\n",
        "\n",
        "plt.figure()\n",
        "for lr in learning_rate:\n",
        "    plt.plot(histories_momentum[lr][1], label=f\"{lr}\")\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26c05bc-8df3-47f4-86c2-4917819e49e7",
      "metadata": {
        "id": "a26c05bc-8df3-47f4-86c2-4917819e49e7"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q6: Compare GD vs Momentum ($β=0.9$) on the same task ($η=0.01$, target (0.5, 1.5)). Plot both the loss curves and the end-effector trajectories side-by-side. Describe one specific, concrete difference in how the arm moves with momentum versus without.\n",
        "</div>\n",
        "<div class=\"alert alert-info\">\n",
        "Q7: Track and plot the magnitude of the velocity vector $||v_t||$ over iterations for the momentum optimizer. What happens to this magnitude as the arm approaches the target? Explain why this behavior occurs.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20dd8d7d",
      "metadata": {
        "id": "20dd8d7d"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "# plt.figure()\n",
        "# plt.plot(histories[lr][2][:, 0], histories[lr][2][:, 1], label=f\"GD\")\n",
        "# plt.plot(histories_momentum[lr][2][:, 0], histories_momentum[lr][2][:, 1], label=f\"Momentum\")\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# plot traj\n",
        "plot_multitraj(target, trajs=[histories[lr][2], histories_momentum[lr][2]], labels=[\"Gradient Desc\", \"Momentum Grad. Desc\"], title=\"Trajectories\")\n",
        "\n",
        "# plot losses\n",
        "\n",
        "# loss is distance sqared\n",
        "distance = 0.01\n",
        "loss_criteria = distance**2\n",
        "# compute iteration needed\n",
        "i = np.arange(0, n_iter+1)\n",
        "i_grad = i[histories[lr][1] > loss_criteria]\n",
        "i_momentum = i[histories_momentum[lr][1] > loss_criteria]\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Losses with all algorithms\")\n",
        "plt.plot(histories[lr][1][histories[lr][1] > loss_criteria], label=f\"Loss for std gradient\", color=\"dodgerblue\")\n",
        "plt.plot(histories_momentum[lr][1][histories_momentum[lr][1] > loss_criteria], label=f\"Loss with momentum\", color=\"darkorange\")\n",
        "plt.axvline(x = i_grad[-1], color = 'dodgerblue', linestyle=\"--\", label = f'Grad cvgd at iter {i_grad[-1]}')\n",
        "plt.axvline(x = i_momentum[-1], color = 'darkorange', linestyle=\"--\", label = f'Momentum cvgd at iter {i_momentum[-1]}')\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A6: The Momentum algorithm converges much faster than the initial one. It takes it about 100 iteration to get within range of target, compared to 500 for the gradient descent algorithm. However, it oscillates a lot when the gradient algorithm does not."
      ],
      "metadata": {
        "id": "ZKt-QvdugwJe"
      },
      "id": "ZKt-QvdugwJe"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(np.linalg.norm(histories_momentum[lr][3][:, :], axis=1), label=f\"Speed norm of Momentum\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qsfhegoiiY5M"
      },
      "id": "qsfhegoiiY5M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7: The velocity decrease when the arm get closer to the target. This is because v is updated as $v_t = βv_{t-1} + ∇L(θ_t)$.\n",
        "\n",
        "When $∇L(θ_t)$ gets small (eg, smaller than $(1-β)v_{t-1}$), the velocity decreases.\n"
      ],
      "metadata": {
        "id": "RZK7VS2ni0yP"
      },
      "id": "RZK7VS2ni0yP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f46884",
      "metadata": {
        "id": "e5f46884"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "v_norm = np.linalg.norm(histories_momentum[lr][3], axis=1)\n",
        "plt.figure()\n",
        "plt.plot(v_norm, label=f\"momentum\")\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b6519f",
      "metadata": {
        "id": "72b6519f"
      },
      "outputs": [],
      "source": [
        "# Run animation\n",
        "ani = animate_robot(histories_momentum[lr])\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123a0bbc-d1f7-4a48-a3f2-31a494da937e",
      "metadata": {
        "id": "123a0bbc-d1f7-4a48-a3f2-31a494da937e"
      },
      "source": [
        "## 2.3: Adam Optimizer\n",
        "\n",
        "Adam (Adaptive Moment Estimation) combines ideas from momentum and adaptive learning rates by maintaining both a moving average of gradients (first moment $m_t$) and a moving average of squared gradients (second moment $v_t$).\n",
        "\n",
        "By dividing the update by $\\sqrt{v_t}$ Adam automatically adjusts the effective learning rate for each parameter based on the history of gradient magnitudes—parameters with large, consistent gradients get smaller effective steps, while parameters with small or noisy gradients get larger effective steps, making it particularly robust across different types of loss landscapes.\n",
        "\n",
        "For this exercise, extend your Momentum SGD to include the second moment, following this update:\n",
        "\n",
        "Update rules:\n",
        "$\n",
        "\\begin{align}\n",
        "g_t &= \\nabla L(\\theta_t) \\\\\n",
        "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
        "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
        "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
        "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
        "\\theta_{t+1} &= \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "\\end{align}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d5a325",
      "metadata": {
        "id": "24d5a325"
      },
      "outputs": [],
      "source": [
        "def Adam(theta_init: np.ndarray,\n",
        "                     target: np.ndarray,\n",
        "                     lengths: np.ndarray,\n",
        "                     learning_rate: float = 0.01,\n",
        "                     n_iterations: int = 500,\n",
        "                     beta1: float = 0.9,\n",
        "                     beta2: float = 0.999,\n",
        "                     lambda_obs: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Convert to JAX arrays\n",
        "    theta = jnp.array(theta_init)\n",
        "    lengths_jax = jnp.array(lengths)\n",
        "    target_jax = jnp.array(target)\n",
        "    v = jnp.array([0, 0])\n",
        "    m = jnp.array([0, 0])\n",
        "    reach_iter = np.inf\n",
        "\n",
        "\n",
        "    loss_fn = lambda th: loss_target_reaching(th, lengths_jax, target_jax)\n",
        "    grad_fn = grad_loss_target\n",
        "\n",
        "    # Storage for history\n",
        "    theta_history = np.zeros((n_iterations + 1, 2))\n",
        "    loss_history = np.zeros(n_iterations + 1)\n",
        "    ee_trajectory = np.zeros((n_iterations + 1, 2))\n",
        "    v_history = np.zeros((n_iterations + 1, 2))\n",
        "    mt_history = np.zeros((n_iterations + 1, 2))\n",
        "\n",
        "    # Initial values\n",
        "    theta_history[0] = np.array(theta)\n",
        "    loss_history[0] = float(loss_fn(theta))\n",
        "    ee_trajectory[0] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "    v_history[0] = np.array(v)\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in range(n_iterations):\n",
        "        # TODO: Compute gradient\n",
        "        # gradient = ?\n",
        "        grad = grad_fn(theta, lengths_jax, target_jax)\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * grad**2\n",
        "        m_hat = m / (1 - beta1**(i + 1))\n",
        "        v_hat = v / (1 - beta2**(i + 1))\n",
        "        # TODO: Update\n",
        "        # theta using gradient descent rule\n",
        "        # theta = ?\n",
        "        theta = theta - learning_rate * m_hat / (jnp.sqrt(v_hat) + 1e-8)\n",
        "        # Store history\n",
        "        theta_history[i + 1] = np.array(theta)\n",
        "        loss_history[i + 1] = float(loss_fn(theta))\n",
        "        ee_trajectory[i + 1] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "        v_history[i+1] = np.array(v)\n",
        "        mt_history[i+1, :] = m\n",
        "\n",
        "        if reach_iter > 1000:\n",
        "            if np.sqrt(np.sum((ee_trajectory[i + 1] - target_jax)**2)) < 1e-2:\n",
        "                reach_iter = i + 1\n",
        "\n",
        "\n",
        "    return theta_history, loss_history, ee_trajectory, v_history, reach_iter, mt_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c61a4f7",
      "metadata": {
        "id": "1c61a4f7"
      },
      "outputs": [],
      "source": [
        "theta0 = np.array([2.5, -2.0])\n",
        "target = np.array([0.8, 0.8])\n",
        "hist_gd = gradient_descent(theta0, target, LENGTHS, learning_rate=0.01, n_iterations=500)\n",
        "histories_momentum = Momentum(theta0, target, LENGTHS, learning_rate=0.01, n_iterations=500)\n",
        "hist_adam = Adam(theta0, target, LENGTHS, learning_rate=0.01, n_iterations=500)\n",
        "\n",
        "plot_multitraj(target, trajs=[hist_gd[2], histories_momentum[2], hist_adam[2]], labels=[\"Gradient Desc\", \"Stoch. Grad. Desc\", \"Adam\"], title=\"Trajectories\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist_gd[2][:, 0], hist_gd[2][:, 1], label=f\"GD\")\n",
        "plt.plot(histories_momentum[2][:, 0], histories_momentum[2][:, 1], label=f\"SGD\")\n",
        "plt.plot(hist_adam[2][:, 0], hist_adam[2][:, 1], label=f\"Adam\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(hist_gd[3])\n",
        "print(histories_momentum[4])\n",
        "print(hist_adam[4])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dceb56b3",
      "metadata": {
        "id": "dceb56b3"
      },
      "outputs": [],
      "source": [
        "# Run animation\n",
        "ani = animate_robot(hist_adam[0])\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f693a0-f27c-4b81-8ed8-6b19906f4021",
      "metadata": {
        "id": "89f693a0-f27c-4b81-8ed8-6b19906f4021"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q8: Start the arm at $θ = [2.5, -2.0]$ trying to reach target ($0.8, 0.8$). Compare how many iterations GD ($η=0.01$), Momentum ($β=0.9, η=0.01$), and Adam (default parameters) each need to get within distance $0.01$ of the target. Report the iteration counts and explain the ranking.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss is distance sqared\n",
        "distance = 0.01\n",
        "loss_criteria = distance**2\n",
        "# compute iteration needed\n",
        "i = np.arange(0, 500+1)\n",
        "i_grad = i[hist_gd[1] > loss_criteria]\n",
        "i_momentum = i[histories_momentum[1] > loss_criteria]\n",
        "i_adam = i[hist_adam[1] > loss_criteria]\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.title(\"Losses with all algorithms\")\n",
        "plt.plot(hist_gd[1][hist_gd[1] > loss_criteria], label=f\"Loss for std gradient\", color=\"dodgerblue\")\n",
        "plt.plot(histories_momentum[1][histories_momentum[1] > loss_criteria], label=f\"Loss with momentum\", color=\"darkorange\")\n",
        "plt.plot(hist_adam[1][hist_adam[1] > loss_criteria], label=f\"Loss for adam\", color=\"crimson\")\n",
        "plt.axvline(x = i_grad[-1], color = 'dodgerblue', linestyle=\"--\", label = f'Grad cvgd at iter {i_grad[-1]}')\n",
        "plt.axvline(x = i_momentum[-1], color = 'darkorange', linestyle=\"--\", label = f'Momentum cvgd at iter {i_momentum[-1]}')\n",
        "plt.axvline(x = i_adam[-1], color = 'crimson', linestyle=\"--\", label = f'Adam cvgd at iter {i_adam[-1]}')\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6UmsUKz8SrOa"
      },
      "id": "6UmsUKz8SrOa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8: The iteration count is displayed on the graph above."
      ],
      "metadata": {
        "id": "sXeYwSXTjn5K"
      },
      "id": "sXeYwSXTjn5K"
    },
    {
      "cell_type": "markdown",
      "id": "b5be9343-5fd0-4359-a3ee-172330f7e319",
      "metadata": {
        "id": "b5be9343-5fd0-4359-a3ee-172330f7e319"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q9: For the Adam optimizer on the same task, plot the effective per-parameter learning rate ($η_eff = η * m̂_t / (√v̂_t + ε)$) for both $θ₁$ and $θ₂$ over time. Do they receive the same effective learning rate throughout optimization? Explain why this adaptive behavior is useful.</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute effective learning rate\n",
        "_, _, _, v_hist, _, m_hist = hist_adam\n",
        "betas = (0.9, 0.999)\n",
        "eps = 1e-8\n",
        "m_hat = m_hist / (1-betas[0]**np.arange(1, 502).reshape((-1, 1)))\n",
        "v_hat = v_hist / (1-betas[1]**np.arange(1, 502).reshape((-1, 1)))\n",
        "eta_eff = 0.01 * m_hat / (np.sqrt(v_hat) + eps)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Effective learning rate per theta\")\n",
        "# plt.plot(loss_hist, label=\"loss\", color=\"lightgrey\")\n",
        "plt.plot(eta_eff[:, 0], label=\"ηeff, theta 1\", color=\"dodgerblue\")\n",
        "plt.plot(eta_eff[:, 1], label=\"ηeff, theta 2\", color=\"darkblue\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Effective learning rate, Loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Etf1ENnksyl"
      },
      "id": "6Etf1ENnksyl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9: The Thetas do not receive the same learning rate over time. This is an interesting behavior, as it allows the arme to dynamically adapt its speed of each joint to the position of the target."
      ],
      "metadata": {
        "id": "_4CzXi6xkdf0"
      },
      "id": "_4CzXi6xkdf0"
    },
    {
      "cell_type": "markdown",
      "id": "b556fcfa-fcf1-4b81-9018-cd87858fcfd8",
      "metadata": {
        "id": "b556fcfa-fcf1-4b81-9018-cd87858fcfd8"
      },
      "source": [
        "## Part 3: Gradient-Free Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4cd6a9-fe47-4f3a-963b-5d7aedef73bb",
      "metadata": {
        "id": "2e4cd6a9-fe47-4f3a-963b-5d7aedef73bb"
      },
      "source": [
        "For this exercise, implement a basic gradient-free algorithm, specifically an evolutionary strategy. Evolution Strategies are a class of gradient-free optimization algorithms inspired by natural evolution. Unlike gradient-based methods that require computing derivatives, ES samples multiple candidate solutions (a \"population\") around the current solution, evaluates their fitness (objective function value), and moves toward the direction indicated by the better-performing samples. Use the following rules for your ES:\n",
        "\n",
        "For iteration $t$, with current parameter vector $\\theta_t$:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\epsilon_i &\\sim \\mathcal{N}(0, I) \\quad \\text{for } i = 1, \\ldots, N \\\\\n",
        "\\theta_i &= \\theta_t + \\sigma \\epsilon_i \\\\\n",
        "F_i &= -L(\\theta_i) \\quad \\text{(fitness = negative loss)} \\\\\n",
        "\\theta_{t+1} &= \\theta_t + \\alpha \\frac{1}{N} \\sum_{i=1}^{N} F_i \\epsilon_i\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $N$ is the population size, $\\sigma$ is the exploration noise (standard deviation), $\\alpha$ is the learning rate, and $\\epsilon_i$ are the perturbation vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8793600-ca32-4b68-8bd6-b22633eca687",
      "metadata": {
        "id": "e8793600-ca32-4b68-8bd6-b22633eca687"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    Q10: Run this ES with population sizes $P = [10, 20, 50]$. For each, report: (a) does it reach the target successfully? (b) how many iterations does it take? (c) total number of function evaluations ($P$ × iterations). What trend do you observe with increasing population size?</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7979b9",
      "metadata": {
        "id": "cc7979b9"
      },
      "outputs": [],
      "source": [
        "def gradient_free_optim(theta_init: np.ndarray,\n",
        "                     target: np.ndarray,\n",
        "                     lengths: np.ndarray,\n",
        "                     learning_rate: float = 0.01,\n",
        "                     n_iterations: int = 500,\n",
        "                     pop_size: int = 10,\n",
        "                     lambda_obs: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Convert to JAX arrays\n",
        "    theta = jnp.array(theta_init)\n",
        "    lengths_jax = jnp.array(lengths)\n",
        "    target_jax = jnp.array(target)\n",
        "    reach_iter = np.inf\n",
        "\n",
        "\n",
        "    loss_fn = lambda th: loss_target_reaching(th, lengths_jax, target_jax)\n",
        "    grad_fn = grad_loss_target\n",
        "\n",
        "    # Storage for history\n",
        "    theta_history = np.zeros((n_iterations + 1, 2))\n",
        "    loss_history = np.zeros(n_iterations + 1)\n",
        "    ee_trajectory = np.zeros((n_iterations + 1, 2))\n",
        "\n",
        "    # Initial values\n",
        "    theta_history[0] = np.array(theta)\n",
        "    loss_history[0] = float(loss_fn(theta))\n",
        "    ee_trajectory[0] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "    # Optimization loop\n",
        "    for i in range(n_iterations):\n",
        "        noise = jax.random.normal(jax.random.PRNGKey(i), (pop_size, 2)) * 0.1\n",
        "        theta_pop = theta + noise\n",
        "        losses = jnp.array([loss_fn(th) for th in theta_pop])\n",
        "        weights = jax.nn.softmax(-losses/0.1)\n",
        "        grad = jnp.sum(weights[:, None] * noise, axis=0)\n",
        "        theta = theta + learning_rate * grad\n",
        "\n",
        "        # Store history\n",
        "        theta_history[i + 1] = np.array(theta)\n",
        "        loss_history[i + 1] = float(loss_fn(theta))\n",
        "        ee_trajectory[i + 1] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
        "        if reach_iter > 1000:\n",
        "            if np.sqrt(np.sum((ee_trajectory[i + 1] - target_jax)**2)) < 1e-2:\n",
        "                reach_iter = i + 1\n",
        "\n",
        "    return theta_history, loss_history, ee_trajectory, reach_iter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab076a4",
      "metadata": {
        "id": "6ab076a4"
      },
      "outputs": [],
      "source": [
        "theta0 = np.array([2.5, -2.0])\n",
        "target = np.array([0.8, 0.8])\n",
        "hist_gd = gradient_descent(theta0, target, LENGTHS, learning_rate=0.01, n_iterations=500)\n",
        "histories_momentum = Momentum(theta0, target, LENGTHS, learning_rate=0.01, n_iterations=500)\n",
        "hist_adam = Adam(theta0, target, LENGTHS, learning_rate=0.01, n_iterations=500)\n",
        "hist_gfo = gradient_free_optim(theta0, target, LENGTHS, learning_rate=0.1, pop_size=50, n_iterations=500)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(hist_gd[2][:, 0], hist_gd[2][:, 1], label=f\"GD\")\n",
        "plt.plot(histories_momentum[2][:, 0], histories_momentum[2][:, 1], label=f\"SGD\")\n",
        "plt.plot(hist_adam[2][:, 0], hist_adam[2][:, 1], label=f\"Adam\")\n",
        "plt.plot(hist_gfo[2][:, 0], hist_gfo[2][:, 1], label=f\"GFO\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Iteration needed for GD \", hist_gd[3])\n",
        "print(\"Iteration needed for Momentum \", histories_momentum[4])\n",
        "print(\"Iteration needed for Adam \", hist_adam[4])\n",
        "print(\"Iteration needed for Genetics\", hist_gfo[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A10:\n",
        "- For p=10, target is reached in 472*10=4720 evaluations\n",
        "- For p=20, target is reached in 424*20 = 10400 evaluations\n",
        "- For p=50, target is reached in 437*50 = 21640 evaluations\n",
        "\n",
        "It seems that when population grows, the trajectory gets less erratic. That makes sens, as a more throughout random exploration allows the algorithm to pick a good candidate at every step."
      ],
      "metadata": {
        "id": "EnOpWtrdnkWq"
      },
      "id": "EnOpWtrdnkWq"
    },
    {
      "cell_type": "markdown",
      "id": "711ad7b9-35c3-484f-b64b-52539ea4541c",
      "metadata": {
        "id": "711ad7b9-35c3-484f-b64b-52539ea4541c"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q11: ES doesn't use gradients, so it evaluates the loss N times per iteration while SGD evaluates it once plus one gradient computation. For a successful run of each method, count total loss function evaluations. Which is more sample-efficient? Despite this, describe one scenario where ES might still be preferred over gradient-based methods.</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A11: ES perfoms N computation, versus 2 for SGD. SGD is therefore more sample efficient. However, ES can be interesting when the gradient cannot be computed easily (eg, when the function is not differentiable)."
      ],
      "metadata": {
        "id": "1J9jimlOqtVE"
      },
      "id": "1J9jimlOqtVE"
    },
    {
      "cell_type": "markdown",
      "id": "a10ce4ad-739b-450e-b18f-4a63500dde1e",
      "metadata": {
        "id": "a10ce4ad-739b-450e-b18f-4a63500dde1e"
      },
      "source": [
        "## Part 4: Trajectory Optimization - Optimizing Sequences\n",
        "\n",
        "So far, we've only optimized the **final configuration** of the robot arm - finding joint angles that place the end-effector at the target. However, in real robotics, we often care about the **entire motion path**. A robot might reach the target, but if it does so with jerky, energy-intensive movements, the solution isn't practical.\n",
        "\n",
        "In this section, you'll optimize a **trajectory** - a sequence of joint angles over time - to reach the target while minimizing energy consumption and ensuring smooth motion.\n",
        "\n",
        "### Trajectory Representation\n",
        "\n",
        "Instead of optimizing $\\theta \\in \\mathbb{R}^2$, we now optimize:\n",
        "$$\\Theta = [\\theta_0, \\theta_1, \\ldots, \\theta_T] \\in \\mathbb{R}^{T \\times 2}$$\n",
        "\n",
        "where $T$ is the number of timesteps (e.g., $T=20$).\n",
        "\n",
        "### Multi-Objective Loss Function\n",
        "\n",
        "We balance three competing objectives:\n",
        "\n",
        "$$L_{total}(\\Theta) = L_{target} + \\lambda_{energy} L_{energy} + \\lambda_{smooth} L_{smooth}$$\n",
        "\n",
        "where:\n",
        "- **Target reaching**: $L_{target} = \\|\\text{pos}(\\theta_T) - \\text{target}\\|^2$ (only final position matters)\n",
        "- **Energy cost**: $L_{energy} = \\sum_{t=1}^{T} \\|\\theta_t - \\theta_{t-1}\\|^2$ (penalize large joint movements)\n",
        "- **Smoothness**: $L_{smooth} = \\sum_{t=2}^{T} \\|(\\theta_t - \\theta_{t-1}) - (\\theta_{t-1} - \\theta_{t-2})\\|^2$ (penalize acceleration/jerkiness)\n",
        "\n",
        "The hyperparameters $\\lambda_{energy}$ and $\\lambda_{smooth}$ control the trade-off between reaching the target quickly versus moving efficiently and smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440a96b9-7a07-467b-a891-aad0c48dfb54",
      "metadata": {
        "id": "440a96b9-7a07-467b-a891-aad0c48dfb54"
      },
      "outputs": [],
      "source": [
        "# Trajectory Optimization Functions\n",
        "\n",
        "def loss_trajectory(theta_sequence: jnp.ndarray,\n",
        "                   lengths: jnp.ndarray,\n",
        "                   target: jnp.ndarray,\n",
        "                   lambda_energy: float = 0.1,\n",
        "                   lambda_smooth: float = 0.05) -> float:\n",
        "    \"\"\"\n",
        "    Loss for trajectory optimization.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta_sequence : jnp.ndarray, shape (T, 2)\n",
        "        Sequence of joint angles over T timesteps\n",
        "    lengths : jnp.ndarray, shape (2,)\n",
        "        Link lengths\n",
        "    target : jnp.ndarray, shape (2,)\n",
        "        Target position\n",
        "    lambda_energy : float\n",
        "        Weight for energy cost\n",
        "    lambda_smooth : float\n",
        "        Weight for smoothness cost\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    loss : float\n",
        "        Total loss combining target reaching, energy, and smoothness\n",
        "    \"\"\"\n",
        "    T = theta_sequence.shape[0]\n",
        "\n",
        "    # Target reaching: only final position matters\n",
        "    final_pos = forward_kinematics_jax(theta_sequence[-1], lengths)\n",
        "    loss_target = jnp.sum((final_pos - target) ** 2)\n",
        "\n",
        "    # Energy cost: sum of squared velocities (joint movements)\n",
        "    velocities = theta_sequence[1:] - theta_sequence[:-1]  # Shape: (T-1, 2)\n",
        "    loss_energy = jnp.sum(velocities ** 2)\n",
        "\n",
        "    # Smoothness cost: sum of squared accelerations\n",
        "    accelerations = velocities[1:] - velocities[:-1]  # Shape: (T-2, 2)\n",
        "    loss_smooth = jnp.sum(accelerations ** 2)\n",
        "\n",
        "    return loss_target + lambda_energy * loss_energy + lambda_smooth * loss_smooth\n",
        "\n",
        "\n",
        "# Create gradient function for trajectory optimization\n",
        "grad_loss_trajectory = jit(grad(loss_trajectory, argnums=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db72287d-557a-4e7e-973d-0c170b0a834a",
      "metadata": {
        "id": "db72287d-557a-4e7e-973d-0c170b0a834a"
      },
      "outputs": [],
      "source": [
        "def plot_trajectory_sequence(theta_sequence: np.ndarray,\n",
        "                             lengths: np.ndarray,\n",
        "                             target: np.ndarray = None,\n",
        "                             title: str = \"Arm Trajectory Over Time\"):\n",
        "    \"\"\"\n",
        "    Visualize the robot arm at multiple points along the trajectory.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta_sequence : np.ndarray, shape (T, 2)\n",
        "        Sequence of joint angles\n",
        "    lengths : np.ndarray\n",
        "        Link lengths\n",
        "    target : np.ndarray, optional\n",
        "        Target position\n",
        "    title : str\n",
        "        Plot title\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "    T = len(theta_sequence)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, T))\n",
        "\n",
        "    # Plot each arm configuration\n",
        "    for t in range(0, T, max(1, T//10)):  # Show ~10 frames\n",
        "        points = get_arm_points(theta_sequence[t], lengths)\n",
        "        alpha = 0.3 + 0.7 * (t / T)  # Fade from transparent to opaque\n",
        "        ax.plot(points[:, 0], points[:, 1], 'o-',\n",
        "               color=colors[t], alpha=alpha, linewidth=2, markersize=4)\n",
        "\n",
        "    # Plot final configuration more prominently\n",
        "    points_final = get_arm_points(theta_sequence[-1], lengths)\n",
        "    ax.plot(points_final[:, 0], points_final[:, 1], 'o-',\n",
        "           color='red', linewidth=3, markersize=8, label='Final')\n",
        "\n",
        "    # Plot end-effector trajectory\n",
        "    ee_traj = np.array([forward_kinematics_numpy(theta_sequence[t], lengths)\n",
        "                        for t in range(T)])\n",
        "    ax.plot(ee_traj[:, 0], ee_traj[:, 1], 'b--',\n",
        "           linewidth=2, alpha=0.5, label='End-effector path')\n",
        "\n",
        "    # Plot target\n",
        "    if target is not None:\n",
        "        ax.plot(target[0], target[1], 'g*', markersize=20, label='Target')\n",
        "\n",
        "    # Plot base\n",
        "    ax.plot(0, 0, 'ks', markersize=12, label='Base')\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(-2.5, 2.5)\n",
        "    ax.set_ylim(-2.5, 2.5)\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec71a92-d796-4b8f-a943-87043fa024fa",
      "metadata": {
        "id": "2ec71a92-d796-4b8f-a943-87043fa024fa"
      },
      "outputs": [],
      "source": [
        "def analyze_trajectory_metrics(theta_sequence: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute and print trajectory quality metrics.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta_sequence : np.ndarray, shape (T, 2)\n",
        "        Sequence of joint angles\n",
        "    \"\"\"\n",
        "    velocities = np.diff(theta_sequence, axis=0)\n",
        "    accelerations = np.diff(velocities, axis=0)\n",
        "\n",
        "    # Compute metrics\n",
        "    total_movement = np.sum(np.abs(velocities))\n",
        "    max_velocity = np.max(np.linalg.norm(velocities, axis=1))\n",
        "    max_acceleration = np.max(np.linalg.norm(accelerations, axis=1))\n",
        "    smoothness = np.sum(accelerations ** 2)  # Lower is smoother\n",
        "\n",
        "    print(\"Trajectory Metrics:\")\n",
        "    print(f\"  Total joint movement: {total_movement:.4f} radians\")\n",
        "    print(f\"  Max velocity: {max_velocity:.4f} rad/timestep\")\n",
        "    print(f\"  Max acceleration: {max_acceleration:.4f} rad/timestep²\")\n",
        "    print(f\"  Smoothness cost: {smoothness:.4f} (lower is better)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f687edf6-fb7a-4eec-9e5e-c6dab976ac1f",
      "metadata": {
        "id": "f687edf6-fb7a-4eec-9e5e-c6dab976ac1f"
      },
      "outputs": [],
      "source": [
        "# Test trajectory optimization with gradient descent\n",
        "def gradient_descent_trajectory(theta_init: np.ndarray,\n",
        "                               target: np.ndarray,\n",
        "                               lengths: np.ndarray,\n",
        "                               T: int = 20,\n",
        "                               learning_rate: float = 0.01,\n",
        "                               n_iterations: int = 1000,\n",
        "                               lambda_energy: float = 0.1,\n",
        "                               lambda_smooth: float = 0.05):\n",
        "    # Initialize trajectory: linear interpolation from init to a guess\n",
        "    target_angles = theta_init + 0.5  # Simple guess for final angles\n",
        "    theta_sequence = jnp.array([\n",
        "        theta_init + (target_angles - theta_init) * t / (T - 1)\n",
        "        for t in range(T)\n",
        "    ])\n",
        "\n",
        "    lengths_jax = jnp.array(lengths)\n",
        "    target_jax = jnp.array(target)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        # Compute loss and gradient\n",
        "        loss_val = loss_trajectory(theta_sequence, lengths_jax, target_jax,\n",
        "                                   lambda_energy, lambda_smooth)\n",
        "        gradient = grad_loss_trajectory(theta_sequence, lengths_jax, target_jax,\n",
        "                                       lambda_energy, lambda_smooth)\n",
        "\n",
        "        # Update trajectory\n",
        "        theta_sequence = theta_sequence - learning_rate * gradient\n",
        "\n",
        "        loss_history.append(float(loss_val))\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            print(f\"Iteration {iteration}: Loss = {loss_val:.6f}\")\n",
        "\n",
        "    return np.array(theta_sequence), np.array(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9738ae3f-d241-4821-87de-e8296d6c59a8",
      "metadata": {
        "id": "9738ae3f-d241-4821-87de-e8296d6c59a8"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "print(\"=\"*60)\n",
        "print(\"Testing Trajectory Optimization\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "theta_init = np.array([0.1, 0.1])\n",
        "target = np.array([1.0, 1.5])\n",
        "T = 20\n",
        "\n",
        "print(f\"\\nOptimizing trajectory with T={T} timesteps\")\n",
        "print(f\"Initial configuration: {theta_init}\")\n",
        "print(f\"Target position: {target}\\n\")\n",
        "\n",
        "theta_traj, loss_hist = gradient_descent_trajectory(\n",
        "    theta_init, target, LENGTHS,\n",
        "    T=T, learning_rate=0.01, n_iterations=1000,\n",
        "    lambda_energy=0.1, lambda_smooth=0.05\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f63af5",
      "metadata": {
        "id": "f3f63af5"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curve(loss_history: np.ndarray, title: str = \"Trajectory Optimization Loss Over Iterations\"):\n",
        "    \"\"\"\n",
        "    Plot the loss curve over iterations.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    loss_history : np.ndarray\n",
        "        Array of loss values over iterations\n",
        "    title : str\n",
        "        Plot title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(loss_history, label='Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss (log scale)')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d192c3-d6b5-4091-97e5-865a9a378b5b",
      "metadata": {
        "id": "88d192c3-d6b5-4091-97e5-865a9a378b5b"
      },
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "plot_loss_curve(loss_hist, \"Trajectory Optimization Loss\")\n",
        "plt.show()\n",
        "\n",
        "plot_trajectory_sequence(theta_traj, LENGTHS, target)\n",
        "plt.show()\n",
        "\n",
        "analyze_trajectory_metrics(theta_traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80097d9-b148-48cd-af42-aa16e8a66f93",
      "metadata": {
        "id": "f80097d9-b148-48cd-af42-aa16e8a66f93"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q12: Run trajectory optimization with three different energy weights: $\\lambda_{energy} = [0.01, 0.1, 1.0]$ (keep $\\lambda_{smooth}=0.05$ fixed). For each, visualize the resulting trajectory and report the total joint movement (sum of velocity magnitudes). How does increasing the energy penalty affect the motion? Does the arm still reach the target?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13087ba-382a-4bbc-949c-c9826dab6e65",
      "metadata": {
        "id": "b13087ba-382a-4bbc-949c-c9826dab6e65"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q13: Compare trajectory optimization using SGD with Momentum vs Adam. Use $T=20$, $\\lambda_{energy}=0.1$, $\\lambda_{smooth}=0.05$. Which converges faster? Plot both trajectories side-by-side. Do they find qualitatively different solutions (different motion strategies)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c352f75-0ce8-42da-8b83-7f995d8836c6",
      "metadata": {
        "id": "4c352f75-0ce8-42da-8b83-7f995d8836c6"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q14: The trajectory optimization problem has $T \\times 2$ parameters (40 parameters for $T=20$). This is similar to optimizing a small neural network. Based on your experiments, which optimizer (GD, Momentum, Adam) would you recommend for optimizing trajectories? How might your answer change if $T=100$ (200 parameters)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6544a73-506c-4401-834b-8a487ddd4002",
      "metadata": {
        "id": "e6544a73-506c-4401-834b-8a487ddd4002"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Q15: Gradient-free methods can also optimize trajectories! However, with $T \\times 2$ parameters, the search space becomes much larger. Implement ES for trajectory optimization. Use a smaller population size initially (N=10) and $T=10$ timesteps to keep computation manageable. Compare sample efficiency (total function evaluations) with Adam. When might ES be preferred despite being less sample-efficient?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d8d351-7432-4142-a28f-77c904502de3",
      "metadata": {
        "id": "98d8d351-7432-4142-a28f-77c904502de3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SDD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}